{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RE.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNLN9anE6MlSAS9bHPqMl7N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JXjexEt7ZbKF"},"source":["# Mount & Installation & Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgurCIYyZLkY","executionInfo":{"status":"ok","timestamp":1616055217399,"user_tz":-480,"elapsed":17433,"user":{"displayName":"huifunny yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVAxxoIwd7W82PHDeewqfoYkiHOdqwTwEF1DLS=s64","userId":"11103776940062567103"}},"outputId":"9794d5fe-52e5-4f47-b9b7-99fa54cfd135"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZYYtFbRZWin","executionInfo":{"status":"ok","timestamp":1616055251190,"user_tz":-480,"elapsed":796,"user":{"displayName":"huifunny yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVAxxoIwd7W82PHDeewqfoYkiHOdqwTwEF1DLS=s64","userId":"11103776940062567103"}},"outputId":"00e859d8-795a-4740-e2a3-5230c462948a"},"source":["cd /content/drive/My Drive/new/ORE"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/new/ORE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qv2YyyN0Zi2l"},"source":["!pip install boto3\r\n","!pip install ipdb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0K9Rfuc9acAZ","executionInfo":{"status":"ok","timestamp":1616055550963,"user_tz":-480,"elapsed":6336,"user":{"displayName":"huifunny yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVAxxoIwd7W82PHDeewqfoYkiHOdqwTwEF1DLS=s64","userId":"11103776940062567103"}},"outputId":"2c28bfce-4e47-4b51-9cbe-f6ddf8a6e08f"},"source":["import os\r\n","import json\r\n","import random\r\n","import pickle\r\n","import argparse\r\n","import requests\r\n","import numpy as np\r\n","from time import time\r\n","from tqdm import tqdm\r\n","from itertools import permutations\r\n","from sklearn.metrics import f1_score, recall_score, precision_score\r\n","import torch\r\n","from bert_codes.pytorch_modeling import BertConfig, BertForQA_CLS\r\n","from bert_codes.pytorch_optimization import get_optimization, warmup_linear\r\n","import bert_codes.entity_tokenization as tokenization\r\n","import bert_codes.utils as utils\r\n","import ipdb"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lclRt-sFaOfs"},"source":["# Configuration"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":644},"id":"AJKzjeuAacub","executionInfo":{"status":"error","timestamp":1616056163693,"user_tz":-480,"elapsed":866,"user":{"displayName":"huifunny yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVAxxoIwd7W82PHDeewqfoYkiHOdqwTwEF1DLS=s64","userId":"11103776940062567103"}},"outputId":"21059af5-ef4f-409c-9086-c5989e701dcb"},"source":["DATA_DIR = \"pretrain_data\"\r\n","MODEL_DIR = \"pretrain_models\"\r\n","\r\n","\r\n","# Configuration\r\n","##########################################################################################\r\n","t_config = time()\r\n","\r\n","# set hyper-parameters\r\n","parser = argparse.ArgumentParser()\r\n","parser.add_argument('--load_train_path', type=str, default=\"data/train_data.json\")  # your path\r\n","parser.add_argument('--load_test_path', type=str, default=\"data/test_data.json\")  # your path\r\n","parser.add_argument('--gpu_ids', type=str, default='0')\r\n","parser.add_argument('--model_name', type=str, default='bert_chinese')  # used pre-trained language model name\r\n","parser.add_argument('--suffix_name', type=str, default='re')  # fine-tuned model suffix name\r\n","\r\n","parser.add_argument('--train_epochs', type=int, default=10)\r\n","parser.add_argument('--n_batch', type=int, default=128)\r\n","parser.add_argument('--class_num', type=int, default=1)  # does relation exist between current entities? yes or no\r\n","parser.add_argument('--lr', type=float, default=5e-5)\r\n","parser.add_argument('--dropout', type=float, default=0.1)\r\n","parser.add_argument('--clip_norm', type=float, default=1.0)\r\n","parser.add_argument('--warmup_rate', type=float, default=0.05)\r\n","parser.add_argument(\"--schedule\", default='warmup_linear', type=str, help='schedule')\r\n","parser.add_argument(\"--weight_decay_rate\", default=0.01, type=float, help='weight_decay_rate')\r\n","parser.add_argument('--seed', type=int, default=42)\r\n","parser.add_argument('--float16', type=bool, default=False)\r\n","parser.add_argument('--eval_steps', type=float, default=0.5)\r\n","parser.add_argument('--save_best', type=bool, default=True)\r\n","parser.add_argument('--vocab_size', type=int, default=21128)\r\n","\r\n","parser.add_argument('--cls_weight', type=str, default=None)  # [a, b], a for pos, b for neg, None for balanced case\r\n","parser.add_argument('--max_seq_length', type=int, default=128)  # maximum sentence length\r\n","parser.add_argument('--max_lines', type=str, default=300000)  # number of lines loaded from the raw corpus\r\n","parser.add_argument('--train_split', type=str, default=0.6)  # probability to choose the sample for train, else for dev\r\n","parser.add_argument('--blank_ratio', type=str, default=0.5)  # probability to mask entity in sentence\r\n","parser.add_argument('--num_relation', type=str,\r\n","                    default=-1)  # maximum number for each relation type from the triples\r\n","parser.add_argument('--repeat_time', type=str, default=[6, 4, 5])  # repeat sampling time for each sentence\r\n","\r\n","parser.add_argument('--train_dir', type=str, default=DATA_DIR)\r\n","parser.add_argument('--dev_dir', type=str, default=DATA_DIR)\r\n","parser.add_argument('--bert_config_file', type=str, default=MODEL_DIR)\r\n","parser.add_argument('--vocab_file', type=str, default=MODEL_DIR)\r\n","parser.add_argument('--init_restore_dir', type=str, default=MODEL_DIR)\r\n","parser.add_argument('--checkpoint_dir', type=str, default='check_points')\r\n","parser.add_argument('--setting_file', type=str, default='setting.txt')\r\n","parser.add_argument('--log_file', type=str, default='log.txt')\r\n","parser.add_argument('--test_log', type=str, default='test_log')\r\n","args = parser.parse_args()\r\n","\r\n","args.train_dir = os.path.join(args.train_dir, args.suffix_name + \"_train.pkl\")\r\n","args.dev_dir = os.path.join(args.dev_dir, args.suffix_name + \"_dev.pkl\")\r\n","args.bert_config_file = os.path.join(args.bert_config_file, args.model_name, 'bert_config.json')\r\n","args.vocab_file = os.path.join(args.vocab_file, args.model_name, 'vocab.txt')\r\n","args.init_restore_dir = os.path.join(args.init_restore_dir, args.model_name, 'pytorch_model.pth')\r\n","args.checkpoint_dir = os.path.join(args.checkpoint_dir, args.model_name + \"_\" + args.suffix_name)\r\n","args.test_log += \"_\" + args.model_name + \"_\" + args.suffix_name + \".txt\"\r\n","args = utils.check_args(args)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--load_train_path LOAD_TRAIN_PATH]\n","                             [--load_test_path LOAD_TEST_PATH]\n","                             [--gpu_ids GPU_IDS] [--model_name MODEL_NAME]\n","                             [--suffix_name SUFFIX_NAME]\n","                             [--train_epochs TRAIN_EPOCHS] [--n_batch N_BATCH]\n","                             [--class_num CLASS_NUM] [--lr LR]\n","                             [--dropout DROPOUT] [--clip_norm CLIP_NORM]\n","                             [--warmup_rate WARMUP_RATE] [--schedule SCHEDULE]\n","                             [--weight_decay_rate WEIGHT_DECAY_RATE]\n","                             [--seed SEED] [--float16 FLOAT16]\n","                             [--eval_steps EVAL_STEPS] [--save_best SAVE_BEST]\n","                             [--vocab_size VOCAB_SIZE]\n","                             [--cls_weight CLS_WEIGHT]\n","                             [--max_seq_length MAX_SEQ_LENGTH]\n","                             [--max_lines MAX_LINES]\n","                             [--train_split TRAIN_SPLIT]\n","                             [--blank_ratio BLANK_RATIO]\n","                             [--num_relation NUM_RELATION]\n","                             [--repeat_time REPEAT_TIME]\n","                             [--train_dir TRAIN_DIR] [--dev_dir DEV_DIR]\n","                             [--bert_config_file BERT_CONFIG_FILE]\n","                             [--vocab_file VOCAB_FILE]\n","                             [--init_restore_dir INIT_RESTORE_DIR]\n","                             [--checkpoint_dir CHECKPOINT_DIR]\n","                             [--setting_file SETTING_FILE]\n","                             [--log_file LOG_FILE] [--test_log TEST_LOG]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-dba95016-05d2-4dd8-bb65-a6824e403811.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  result : :class:`ExecutionResult`\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"y-YG_tEvZoVu","executionInfo":{"status":"error","timestamp":1616056167191,"user_tz":-480,"elapsed":776,"user":{"displayName":"huifunny yang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVAxxoIwd7W82PHDeewqfoYkiHOdqwTwEF1DLS=s64","userId":"11103776940062567103"}},"outputId":"27a2e828-f3e0-46a2-94e0-9c1d0847622b"},"source":["# bert initialization\r\n","bert_config = BertConfig.from_json_file(args.bert_config_file)\r\n","tokenizer = tokenization.BertTokenizer(vocab_file=args.vocab_file, do_lower_case=True)\r\n","model = BertForQA_CLS(config=bert_config, num_labels=args.class_num)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-68cf665ed18a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# bert initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQA_CLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_KU27hOWZvBV"},"source":["\r\n","\r\n","\r\n","\r\n","\r\n","\r\n","# set seed\r\n","random.seed(args.seed)\r\n","np.random.seed(args.seed)\r\n","torch.manual_seed(args.seed)\r\n","\r\n","# set gpu\r\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_ids\r\n","device = torch.device(\"cuda\")\r\n","is_cuda = True\r\n","n_gpu = torch.cuda.device_count()\r\n","if n_gpu > 0:\r\n","    torch.cuda.manual_seed_all(args.seed)\r\n","\r\n","# initialize model\r\n","print('init model...')\r\n","utils.torch_show_all_params(model)\r\n","utils.torch_init_model(model, args.init_restore_dir)  # load the saved model according to the checkpoint_dir when prediction\r\n","if args.float16:\r\n","    model.half()\r\n","model.to(device)\r\n","if n_gpu > 1:\r\n","    model = torch.nn.DataParallel(model)\r\n","\r\n","print(\"Initial Configuaration Time: {}\".format(time() - t_config))\r\n","\r\n","\r\n","# Data Preparation (in training step)\r\n","##########################################################################################\r\n","def kmp_match(s1, s2):\r\n","    def gen_next(s):\r\n","        k = -1\r\n","        n = len(s)\r\n","        m = 0\r\n","        lst = [0] * n\r\n","        lst[0] = -1\r\n","        while m < n - 1:\r\n","            if k == -1 or s[k] == s[m]:\r\n","                k += 1\r\n","                m += 1\r\n","                lst[m] = k\r\n","            else:\r\n","                k = lst[k]\r\n","        return lst\r\n","\r\n","    next_list = gen_next(s2)\r\n","    ans = -1\r\n","    i = 0\r\n","    j = 0\r\n","    while i < len(s1):\r\n","        if s1[i] == s2[j] or j == -1:\r\n","            i += 1\r\n","            j += 1\r\n","        else:\r\n","            j = next_list[j]\r\n","        if j == len(s2):\r\n","            ans = i - len(s2)\r\n","            break\r\n","    return ans\r\n","\r\n","\r\n","def add_tag(span_ids, span_type, input_ids=None, input_ids_mask=None, blank_ratio=-1):\r\n","    # [42] for <entity_head_begin>, [43] for <entity_head_end>,\r\n","    # [44] for <entity_tail_begin>, [45] for <entity_tail_end>,\r\n","    # [13] for <blank>, [-1] for <mask_span>\r\n","    # assert\r\n","    if input_ids is None or input_ids_mask is None:\r\n","        return None\r\n","    # find begin and end indexes\r\n","    idx_begin = kmp_match(input_ids_mask, span_ids)\r\n","    if idx_begin == -1:\r\n","        return None\r\n","    idx_end = idx_begin + len(span_ids)\r\n","    # define tags\r\n","    span_tag = [-1]\r\n","    blank_tag = [13]\r\n","    blank_mode = False\r\n","    if \"head\" in span_type:\r\n","        fore_tag, post_tag = [42], [43]\r\n","    if \"tail\" in span_type:\r\n","        fore_tag, post_tag = [44], [45]\r\n","    # add tags to entity\r\n","    if \"head\" in span_type or \"tail\" in span_type:\r\n","        input_ids_mask = input_ids[:idx_begin] + span_tag * (len(span_ids) + 2) + input_ids[idx_end:]\r\n","        if random.random() > blank_ratio:  # blank_ratio == -1 means tagging without blank\r\n","            input_ids = input_ids[:idx_begin] + fore_tag + input_ids[idx_begin:idx_end] + post_tag + input_ids[idx_end:]\r\n","            blank_mode = True\r\n","        else:\r\n","            input_ids = input_ids[:idx_begin] + fore_tag + blank_tag * len(span_ids) + post_tag + input_ids[idx_end:]\r\n","    if len(input_ids_mask) != len(input_ids):\r\n","        raise ValueError(\"[ERROR] Lengths of input_ids and input_ids_mask should be equal.\")\r\n","    return idx_begin, idx_end, input_ids, input_ids_mask, blank_mode\r\n","\r\n","\r\n","def get_input_ids(tokenizer, text, entity_head, entity_tail, relation=None, max_seq_length=128, blank_ratio=0.5,\r\n","                  is_check=None):\r\n","    assert isinstance(entity_head, str) and isinstance(entity_head, str)\r\n","    assert len(text) > 0 and len(entity_head) > 0 and len(entity_tail) > 0\r\n","    # get tokens\r\n","    lst_text = tokenizer.tokenize(text)\r\n","    lst_entity_head = tokenizer.tokenize(entity_head)\r\n","    lst_entity_tail = tokenizer.tokenize(entity_tail)\r\n","    # cut over-length tokens\r\n","    if len(\r\n","            lst_text) > max_seq_length - 6:  # 6指的是[CLS]、[SEP]、<entity_head_begin>、<entity_head_end>、<entity_tail_begin>、<entity_tail_end>\r\n","        lst_text = lst_text[:max_seq_length - 6]\r\n","    lst_text = [\"[CLS]\"] + lst_text + [\"[SEP]\"]\r\n","    # token to ids\r\n","    input_ids = tokenizer.convert_tokens_to_ids(lst_text)\r\n","    input_head_ids = tokenizer.convert_tokens_to_ids(lst_entity_head)\r\n","    input_tail_ids = tokenizer.convert_tokens_to_ids(lst_entity_tail)\r\n","    # initialize blank_mode\r\n","    blank_mode = [0, 0]\r\n","    # add tags according to the order\r\n","    if len(input_head_ids) >= len(input_tail_ids):\r\n","        res = add_tag(input_head_ids, \"head\", input_ids, input_ids, blank_ratio=blank_ratio)\r\n","        if res:\r\n","            blank_mode[0] = 1 if res[-1] else 0\r\n","            res = add_tag(input_tail_ids, \"tail\", res[2], res[3], blank_ratio=blank_ratio)\r\n","            if res:\r\n","                blank_mode[1] = 1 if res[-1] else 0\r\n","                input_ids, input_ids_mask = res[2], res[3]\r\n","            else:\r\n","                return None, -1, -1, blank_mode  # entity_tail not exist\r\n","        else:\r\n","            return None, -1, -1, blank_mode  # entity_head not exist\r\n","    else:\r\n","        res = add_tag(input_tail_ids, \"tail\", input_ids, input_ids, blank_ratio=blank_ratio)\r\n","        if res:\r\n","            blank_mode[1] = 1 if res[-1] else 0\r\n","            res = add_tag(input_head_ids, \"head\", res[2], res[3], blank_ratio=blank_ratio)\r\n","            if res:\r\n","                blank_mode[0] = 1 if res[-1] else 0\r\n","                input_ids, input_ids_mask = res[2], res[3]\r\n","            else:\r\n","                return None, -1, -1, blank_mode  # entity_head not exist\r\n","        else:\r\n","            return None, -1, -1, blank_mode  # entity_tail not exist\r\n","    # find relation_span\r\n","    idx_relation_begin, idx_relation_end = 0, 0\r\n","    if isinstance(relation, str) and len(relation) > 0:\r\n","        lst_relation = tokenizer.tokenize(relation)\r\n","        input_relation_ids = tokenizer.convert_tokens_to_ids(lst_relation)\r\n","        res = add_tag(input_relation_ids, \"relation\", input_ids, input_ids_mask, blank_ratio=blank_ratio)\r\n","        if res:\r\n","            idx_relation_begin, idx_relation_end = res[0], res[1]\r\n","        else:\r\n","            return None, -1, -1, blank_mode  # relation cannot be found\r\n","    # padding\r\n","    while len(input_ids) < max_seq_length:\r\n","        input_ids.append(0)\r\n","    if len(input_ids) > max_seq_length:\r\n","        raise ValueError(\"[ERROR] input_ids should be shorter than max_seq_length.\")\r\n","    # check\r\n","    if is_check:\r\n","        print(is_check)\r\n","        print(\"text:\", \"\".join(tokenizer.convert_ids_to_tokens(input_ids[:input_ids.index(102)])))\r\n","        print(\"relation:\", \"\".join(tokenizer.convert_ids_to_tokens(input_ids[idx_relation_begin:idx_relation_end])))\r\n","    return input_ids, idx_relation_begin, idx_relation_end, blank_mode\r\n","\r\n","\r\n","def raw2json(tokenizer, load_path, save_path=None, max_lines=100, max_seq_length=128, train_split=0.95, print_time=100,\r\n","             blank_ratio=0.4, num_relation=-1, repeat_time=[1, 1, 1]):\r\n","    global DATA_DIR\r\n","    features_train = list()\r\n","    features_dev = list()\r\n","    unique_id = 0  # count samples\r\n","    c_pos_tuple = 0  # count positive samples without relation\r\n","    c_pos_triple = 0  # count positive samples with relation\r\n","    c_neg = 0  # count negative samples\r\n","    dict_relation = dict()  # record relation types\r\n","\r\n","    with open(load_path, \"r\") as f:\r\n","        for i_line, line in enumerate(f):\r\n","            if i_line > max_lines:  # control the number of operated samples\r\n","                break\r\n","            line_now = json.loads(line)\r\n","            lst_samples = line_now.get(\"EL_res\")\r\n","            for sample in lst_samples:\r\n","                # choose train or dev by probability\r\n","                is_dev = -1 if random.random() > train_split else blank_ratio  # dev samples should not be blanked\r\n","                repeat_time_now = [1, 1, 1] if is_dev == -1 else repeat_time  # dev samples could not be repeated\r\n","\r\n","                # print\r\n","                if i_line % print_time == 0:\r\n","                    print(\"-\" * 50)\r\n","                    print(\"* pos_triple-{} + pos_tuple-{} + neg-{} = {} samples from {} lines.\".format(c_pos_triple,\r\n","                                                                                                       c_pos_tuple,\r\n","                                                                                                       c_neg, unique_id,\r\n","                                                                                                       i_line))\r\n","                # pre-processing entities\r\n","                text = sample.get(\"text\")\r\n","                facts = sample.get(\"triples\")\r\n","                lst_entities = list(sample.get(\"entity_idx\").keys())  # all entities\r\n","                lst_pair = [(h, t) for h, t in permutations(lst_entities, 2) if h != t]\r\n","                lst_pos_triple = list()\r\n","                lst_pos_tuple = list()\r\n","                for fact in facts:\r\n","                    if fact[-1]:  # a pos_triple\r\n","                        lst_pos_triple.append(fact)\r\n","                        lst_pair.remove((fact[0], fact[1]))\r\n","                    elif fact[1]:  # a pos_tuple\r\n","                        lst_pos_tuple.append(fact)\r\n","                        lst_pair.remove((fact[0], fact[1]))\r\n","                    else:  # a neg entity\r\n","                        continue\r\n","                if len(lst_pos_triple) == 0:  # there is no pos_triple in this line\r\n","                    continue\r\n","                random.shuffle(lst_pair)\r\n","\r\n","                # get positive triples\r\n","                n_valid_triple = 0\r\n","                for i_fact, fact in enumerate(lst_pos_triple):\r\n","                    # balance relation type\r\n","                    if num_relation > 0:  # otherwise no constraints\r\n","                        if fact[-1] in dict_relation.keys():\r\n","                            if dict_relation[fact[-1]] > num_relation:\r\n","                                continue\r\n","                            else:\r\n","                                dict_relation[fact[-1]] = dict_relation[fact[-1]] + 1\r\n","                        else:\r\n","                            dict_relation[fact[-1]] = 1\r\n","                    # calculate triples\r\n","                    check_tag = \"positive_triple:\" if i_fact < 5 else None\r\n","                    lst_mode = list()\r\n","                    for _ in range(repeat_time_now[0]):\r\n","                        input_ids, label_start, label_end, blank_mode = get_input_ids(tokenizer, text, fact[0], fact[1],\r\n","                                                                                      relation=fact[2],\r\n","                                                                                      max_seq_length=max_seq_length,\r\n","                                                                                      blank_ratio=is_dev,\r\n","                                                                                      is_check=check_tag)\r\n","                        if input_ids and blank_mode not in lst_mode:\r\n","                            feature = {\r\n","                                'unique_id': unique_id,\r\n","                                'input_ids': input_ids,\r\n","                                'label_start': label_start,\r\n","                                'label_end': label_end,\r\n","                                'label_class': 1}\r\n","                            if is_dev < 0:\r\n","                                features_dev.append(feature)\r\n","                            else:\r\n","                                features_train.append(feature)\r\n","                            unique_id += 1\r\n","                            c_pos_triple += 1\r\n","                            lst_mode.append(blank_mode)\r\n","                            n_valid_triple += 1\r\n","                            print(\"[INSERT] OK.\")\r\n","                            print()\r\n","                        else:\r\n","                            print(\"[INSERT] FAILED.\")\r\n","                            print()\r\n","                    if len(lst_mode) > 0:\r\n","                        n_valid_triple = n_valid_triple + 1 - len(lst_mode)  # delete sample triples\r\n","\r\n","                # get positive tuples\r\n","                for j_fact, fact in enumerate(lst_pos_tuple):\r\n","                    if n_valid_triple == 0:\r\n","                        if random.random() > 0.5:\r\n","                            break\r\n","                    if j_fact == max(int(round(0.5 * n_valid_triple)), 1):  # pos_tuple = pos_triple\r\n","                        break\r\n","                    # calculate tuples\r\n","                    check_tag = \"positive_tuple:\" if i_fact < 5 else None\r\n","                    lst_mode = list()\r\n","                    for _ in range(repeat_time_now[1]):\r\n","                        input_ids, label_start, label_end, blank_mode = get_input_ids(tokenizer, text, fact[0], fact[1],\r\n","                                                                                      max_seq_length=max_seq_length,\r\n","                                                                                      blank_ratio=is_dev,\r\n","                                                                                      is_check=check_tag)\r\n","                        if input_ids and blank_mode not in lst_mode:\r\n","                            feature = {\r\n","                                'unique_id': unique_id,\r\n","                                'input_ids': input_ids,\r\n","                                'label_start': label_start,\r\n","                                'label_end': label_end,\r\n","                                'label_class': 1}\r\n","                            if is_dev < 0:\r\n","                                features_dev.append(feature)\r\n","                            else:\r\n","                                features_train.append(feature)\r\n","                            unique_id += 1\r\n","                            c_pos_tuple += 1\r\n","                            lst_mode.append(blank_mode)\r\n","                            print(\"[INSERT] OK.\")\r\n","                            print()\r\n","                        else:\r\n","                            print(\"[INSERT] FAILED.\")\r\n","                            print()\r\n","\r\n","                # get negative tuples\r\n","                for k_fact, fact in enumerate(lst_pair):\r\n","                    if k_fact == max(int(round(0.5 * n_valid_triple)), 1):  # neg_tuple = pos_tuple + pos_triple\r\n","                        break\r\n","                    # calculate negative samples\r\n","                    check_tag = \"negative_tuple:\" if i_fact < 5 else None\r\n","                    lst_mode = list()\r\n","                    for _ in range(repeat_time_now[2]):\r\n","                        input_ids, label_start, label_end, blank_mode = get_input_ids(tokenizer, text, fact[0], fact[1],\r\n","                                                                                      max_seq_length=max_seq_length,\r\n","                                                                                      blank_ratio=is_dev,\r\n","                                                                                      is_check=check_tag)\r\n","                        if input_ids and blank_mode not in lst_mode:\r\n","                            feature = {\r\n","                                'unique_id': unique_id,\r\n","                                'input_ids': input_ids,\r\n","                                'label_start': label_start,\r\n","                                'label_end': label_end,\r\n","                                'label_class': 0}\r\n","                            if is_dev < 0:\r\n","                                features_dev.append(feature)\r\n","                            else:\r\n","                                features_train.append(feature)\r\n","                            unique_id += 1\r\n","                            c_neg += 1\r\n","                            lst_mode.append(blank_mode)\r\n","                            print(\"[INSERT] OK.\")\r\n","                            print()\r\n","                        else:\r\n","                            print(\"[INSERT] FAILED.\")\r\n","                            print()\r\n","                print(\"-\" * 50)\r\n","\r\n","    # save data\r\n","    if save_path:\r\n","        if not os.path.isdir(DATA_DIR):\r\n","            os.mkdir(DATA_DIR)\r\n","        with open(os.path.join(DATA_DIR, save_path + \"_train.pkl\"), 'wb') as fw:\r\n","            pickle.dump(features_train, fw)\r\n","        with open(os.path.join(DATA_DIR, save_path + \"_dev.pkl\"), 'wb') as fw:\r\n","            pickle.dump(features_dev, fw)\r\n","        print(\"Train size-{}: Dev size-{}\".format(len(features_train), len(features_dev)))\r\n","        print(\"Final pos_triple-{} + pos_tuple-{} + neg-{} = {} samples.\".format(c_pos_triple, c_pos_tuple, c_neg,\r\n","                                                                                 unique_id))\r\n","        if dict_relation:\r\n","            print(\"Number of relation types-{}, and average numbers-{}\".format(len(dict_relation), sum(\r\n","                [v for _, v in dict_relation.items()]) / float(len(dict_relation))))\r\n","        else:\r\n","            print(\"Sizes among relation types are imbalanced.\")\r\n","        return True\r\n","    else:\r\n","        return features_train, features_dev\r\n","\r\n","\r\n","# Data Preparation (in test step)\r\n","##########################################################################################\r\n","def add_tag_test(span_ids, span_type, input_ids=None, input_ids_mask=None):\r\n","    # [42] for <entity_head_begin>, [43] for <entity_head_end>,\r\n","    # [44] for <entity_tail_begin>, [45] for <entity_tail_end>,\r\n","    # [13] for <blank>, [-1] for <mask_span>\r\n","    # assert\r\n","    if input_ids is None or input_ids_mask is None:\r\n","        return None\r\n","    # find begin and end indexes\r\n","    idx_begin = kmp_match(input_ids_mask, span_ids)\r\n","    if idx_begin == -1:\r\n","        return None\r\n","    idx_end = idx_begin + len(span_ids)\r\n","    # define tags\r\n","    span_tag = [-1]\r\n","    if \"head\" in span_type:\r\n","        fore_tag, post_tag = [42], [43]\r\n","    if \"tail\" in span_type:\r\n","        fore_tag, post_tag = [44], [45]\r\n","    # add tags to entity\r\n","    if \"head\" in span_type or \"tail\" in span_type:\r\n","        input_ids_mask = input_ids[:idx_begin] + span_tag * (len(span_ids) + 2) + input_ids[idx_end:]\r\n","        input_ids = input_ids[:idx_begin] + fore_tag + input_ids[idx_begin:idx_end] + post_tag + input_ids[idx_end:]\r\n","    if len(input_ids_mask) != len(input_ids):\r\n","        raise ValueError(\"[ERROR] Lengths of input_ids and input_ids_mask should be equal.\")\r\n","    return idx_begin, idx_end, input_ids, input_ids_mask\r\n","\r\n","\r\n","def get_input_ids_test(tokenizer, text, entity_head, entity_tail, max_seq_length=128):\r\n","    assert isinstance(entity_head, str) and isinstance(entity_tail, str)\r\n","    assert len(text) > 0 and len(entity_head) > 0 and len(entity_tail) > 0\r\n","    # get tokens\r\n","    lst_text = tokenizer.tokenize(text)\r\n","    lst_entity_head = tokenizer.tokenize(entity_head)\r\n","    lst_entity_tail = tokenizer.tokenize(entity_tail)\r\n","    # cut over-length tokens\r\n","    if len(lst_text) > max_seq_length - 6:\r\n","        lst_text = lst_text[:max_seq_length - 6]\r\n","    lst_text = [\"[CLS]\"] + lst_text + [\"[SEP]\"]\r\n","    # token to ids\r\n","    input_ids = tokenizer.convert_tokens_to_ids(lst_text)\r\n","    input_head_ids = tokenizer.convert_tokens_to_ids(lst_entity_head)\r\n","    input_tail_ids = tokenizer.convert_tokens_to_ids(lst_entity_tail)\r\n","    # add tags according to the order\r\n","    output_ids = None\r\n","    if len(input_head_ids) >= len(input_tail_ids):\r\n","        res = add_tag_test(input_head_ids, \"head\", input_ids, input_ids)\r\n","        if res:\r\n","            res = add_tag_test(input_tail_ids, \"tail\", res[2], res[3])\r\n","            if res:\r\n","                output_ids = res[2]\r\n","    else:\r\n","        res = add_tag_test(input_tail_ids, \"tail\", input_ids, input_ids)\r\n","        if res:\r\n","            res = add_tag_test(input_head_ids, \"head\", res[2], res[3])\r\n","            if res:\r\n","                output_ids = res[2]\r\n","    # padding\r\n","    if output_ids:\r\n","        while len(output_ids) < max_seq_length:\r\n","            output_ids.append(0)\r\n","        if len(output_ids) > max_seq_length:\r\n","            raise ValueError(\"[ERROR] input_ids should be shorter than max_seq_length.\")\r\n","    return output_ids\r\n","\r\n","\r\n","def string2token(tokenizer, text, lst_entities, max_seq_length=128):\r\n","    # text: string, the input sentence\r\n","    # lst_entities: list, the list of cadidate entities\r\n","    if len(lst_entities) <= 1 or len(text) == 0:\r\n","        return None, None\r\n","    features = list()\r\n","    max_seq_len = 0\r\n","    for entity_head, entity_tail in permutations(lst_entities, 2):\r\n","        input_ids = get_input_ids_test(tokenizer, text, entity_head, entity_tail, max_seq_length=max_seq_length)\r\n","        if input_ids:\r\n","            features.append({'input_ids': input_ids})\r\n","            if len(input_ids) > max_seq_len:\r\n","                max_seq_len = len(input_ids)\r\n","    return features, max_seq_len\r\n","\r\n","\r\n","# Batch Generation\r\n","##########################################################################################\r\n","class GenData(object):\r\n","    def __init__(self, batch_size, is_cuda, data_dir, is_train=True):\r\n","        with open(os.path.join(data_dir), \"rb\") as f:\r\n","            self.all_data = pickle.load(f)\r\n","        self.batch_size = batch_size\r\n","        self.is_train = is_train\r\n","        self.cuda = is_cuda\r\n","        self.data = GenData.make_baches(self.all_data, self.batch_size, self.is_train)\r\n","        self.offset = 0\r\n","\r\n","    @staticmethod\r\n","    def make_baches(data, batch_size=32, is_train=True):\r\n","        if is_train:\r\n","            random.shuffle(data)\r\n","            return [data[i: i + batch_size] if i + batch_size < len(data) else data[i:] + data[\r\n","                                                                                          :i + batch_size - len(\r\n","                                                                                              data)] for i in\r\n","                    range(0, len(data), batch_size)]\r\n","        # 确保多gpu推断正常运作\r\n","        return [data[i: i + batch_size] if i + batch_size < len(data) else data[i:] + data[\r\n","                                                                                      :i + batch_size - len(\r\n","                                                                                          data)] for i in\r\n","                range(0, len(data), batch_size)]\r\n","\r\n","    def reset(self):\r\n","        if self.is_train:\r\n","            self.data = GenData.make_baches(self.all_data, self.batch_size, self.is_train)\r\n","        self.offset = 0\r\n","\r\n","    def __len__(self):\r\n","        return len(self.data)\r\n","\r\n","    def __iter__(self):\r\n","        while self.offset < len(self):\r\n","            batch = self.data[self.offset]\r\n","            self.offset += 1\r\n","            bsz = len(batch)\r\n","            max_seq_len = max([len(sample['input_ids']) for sample in batch])  # 每个batch中sequence长度对齐\r\n","            # passage inputs\r\n","            input_ids = torch.LongTensor([sample['input_ids'] for sample in batch])[:, :max_seq_len]\r\n","            input_mask = torch.LongTensor([[1] * len(sample['input_ids']) for sample in batch])[:, :max_seq_len]\r\n","            input_segments = torch.LongTensor([[0] * len(sample['input_ids']) for sample in batch])[:,\r\n","                             :max_seq_len]\r\n","            label_start = torch.LongTensor([sample['label_start'] for sample in batch])\r\n","            label_end = torch.LongTensor([sample['label_end'] for sample in batch])\r\n","            label_class = torch.FloatTensor([sample['label_class'] for sample in batch])\r\n","            out_batch = {\r\n","                \"input_ids\": input_ids,\r\n","                \"input_mask\": input_mask,\r\n","                \"input_segments\": input_segments,\r\n","                \"label_start\": label_start,\r\n","                \"label_end\": label_end,\r\n","                \"label_class\": label_class\r\n","            }\r\n","            if self.cuda:\r\n","                for k in out_batch.keys():\r\n","                    if isinstance(out_batch[k], torch.Tensor):\r\n","                        out_batch[k] = out_batch[k].cuda()\r\n","            yield out_batch\r\n","\r\n","\r\n","def gen_batch_test(input_data, max_seq_len, is_cuda):\r\n","    input_ids = torch.LongTensor([sample['input_ids'] for sample in input_data])[:, :max_seq_len]\r\n","    input_mask = torch.LongTensor([[1] * len(sample['input_ids']) for sample in input_data])[:,\r\n","                 :max_seq_len]\r\n","    input_segments = torch.LongTensor([[0] * len(sample['input_ids']) for sample in input_data])[:,\r\n","                     :max_seq_len]\r\n","    out_batch = {\r\n","        \"input_ids\": input_ids,\r\n","        \"input_mask\": input_mask,\r\n","        \"input_segments\": input_segments}\r\n","    if is_cuda:\r\n","        for k in out_batch.keys():\r\n","            if isinstance(out_batch[k], torch.Tensor):\r\n","                out_batch[k] = out_batch[k].cuda()\r\n","    return out_batch\r\n","\r\n","\r\n","# Training (in training step)\r\n","##########################################################################################\r\n","def train(args=None, tokenizer=None, model=None, is_cuda=None, n_gpu=None):\r\n","    # set data generators for train and dev\r\n","    train_data_gen = GenData(args.n_batch, is_cuda, args.train_dir, is_train=True)\r\n","    dev_data_gen = GenData(args.n_batch, is_cuda, args.dev_dir, is_train=False)\r\n","    if os.path.exists(args.log_file):\r\n","        os.remove(args.log_file)\r\n","\r\n","    # set training steps\r\n","    steps_per_epoch = len(train_data_gen)\r\n","    args.eval_steps = int(args.eval_steps * steps_per_epoch)\r\n","    total_steps = steps_per_epoch * args.train_epochs\r\n","    print(\"steps per epoch: {}; total steps: {}; warmup steps: {}\"\r\n","          .format(steps_per_epoch, total_steps, int(args.warmup_rate * total_steps)))\r\n","\r\n","    # set optimizer\r\n","    optimizer = get_optimization(model=model, float16=args.float16, learning_rate=args.lr, total_steps=total_steps,\r\n","                                 schedule=args.schedule, warmup_rate=args.warmup_rate, max_grad_norm=args.clip_norm,\r\n","                                 weight_decay_rate=args.weight_decay_rate)\r\n","\r\n","    print('***** Training *****')\r\n","    global_steps = 1\r\n","    best_f1 = 0\r\n","    best_acc = 0\r\n","    cls_weight_now = torch.Tensor(args.cls_weight).cuda() if args.cls_weight else None\r\n","    for i in range(int(args.train_epochs)):\r\n","        print('Starting epoch {}'.format(i + 1))\r\n","        model.train()\r\n","        train_data_gen.reset()\r\n","        total_loss = 0\r\n","        iteration = 1\r\n","        with tqdm(total=steps_per_epoch, desc='Epoch %d' % (i + 1), ncols=50) as pbar:\r\n","            for step, batch in enumerate(train_data_gen):\r\n","                loss = model(input_ids=batch['input_ids'],\r\n","                             token_type_ids=batch['input_segments'],\r\n","                             attention_mask=batch['input_mask'],\r\n","                             start_positions=batch['label_start'],\r\n","                             end_positions=batch['label_end'],\r\n","                             target_labels=batch['label_class'],\r\n","                             cls_weight=cls_weight_now)\r\n","                if n_gpu > 1:\r\n","                    loss = loss.mean()  # mean() to average on multi-gpu.\r\n","                total_loss += loss.item()\r\n","                pbar.set_postfix({'loss': '{0:1.5f}'.format(total_loss / (iteration + 1e-5))})\r\n","                pbar.update(1)\r\n","\r\n","                if args.float16:\r\n","                    optimizer.backward(loss)\r\n","                    lr_this_step = args.lr * warmup_linear(global_steps / total_steps, args.warmup_rate)\r\n","                    for param_group in optimizer.param_groups:\r\n","                        param_group['lr'] = lr_this_step\r\n","                else:\r\n","                    loss.backward()\r\n","\r\n","                optimizer.step()\r\n","                model.zero_grad()\r\n","                global_steps += 1\r\n","                iteration += 1\r\n","\r\n","                if global_steps % args.eval_steps == 0:\r\n","                    f1, acc = evaluate(tokenizer, model, dev_data_gen)\r\n","                    with open(args.log_file, 'a') as aw:\r\n","                        aw.write('global steps:{}, f1:{}, acc:{}'\r\n","                                 .format(global_steps, f1, acc) + '\\n')\r\n","                    print('global steps:{}, f1:{}, acc:{}'.format(global_steps, f1, acc))\r\n","                    if f1 > best_f1 or acc > best_acc:\r\n","                        if f1 > best_f1:\r\n","                            best_f1 = f1\r\n","                        if acc > best_acc:\r\n","                            best_acc = acc\r\n","                        utils.torch_save_model(model, args.checkpoint_dir, {'f1': f1, 'acc': acc}, max_save_num=1)\r\n","                    model.train()\r\n","    return None\r\n","\r\n","\r\n","# Evaluation (in training step)\r\n","##########################################################################################\r\n","def evaluate(tokenizer, model, dev_data_gen):\r\n","    print(\"***** Eval *****\")\r\n","    model.eval()\r\n","    dev_data_gen.reset()\r\n","    f1_all = 0.0\r\n","    acc_all = 0.0\r\n","    with torch.no_grad():\r\n","        for i_batch, batch in enumerate(dev_data_gen):\r\n","            start_logits, end_logits, target_logits = model(input_ids=batch['input_ids'],\r\n","                                                            token_type_ids=batch['input_segments'],\r\n","                                                            attention_mask=batch['input_mask'])\r\n","            # get predicted labels\r\n","            start_logits = start_logits.detach().cpu().numpy()  # [bs, len]\r\n","            end_logits = end_logits.detach().cpu().numpy()  # [bs, len]\r\n","            start_pre = np.argmax(start_logits, axis=-1)  # [bs,]\r\n","            end_pre = np.argmax(end_logits, axis=-1)  # [bs,]\r\n","            class_pre = target_logits.detach().cpu().numpy()  # [bs,]\r\n","            class_pre[class_pre <= 0.5] = 0\r\n","            class_pre[class_pre > 0] = 1\r\n","            # get true labels\r\n","            start_true = batch['label_start'].detach().cpu().numpy()\r\n","            end_true = batch['label_end'].detach().cpu().numpy()\r\n","            class_true = batch['label_class'].detach().cpu().numpy()\r\n","            # calculate each sample in the batch\r\n","            f1 = 0.0\r\n","            batch_size = target_logits.shape[0]\r\n","            for i in range(batch_size):\r\n","                lst_text = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][i].detach().cpu().tolist())\r\n","                # get predicted or true relations\r\n","                if start_true[i] == end_true[i]:  # non-span\r\n","                    if start_pre[i] == start_true[i] and end_pre[i] == end_true[i]:\r\n","                        f1 += 1.0\r\n","                else:  # span\r\n","                    relation_true = lst_text[start_true[i]:end_true[i]]\r\n","                    relation_pre = lst_text[start_pre[i]:end_pre[i]]\r\n","                    print(\"compare(true/pre): {} / {}\".format(relation_true, relation_pre))\r\n","                    # calculate metrics\r\n","                    correct = len(set(relation_true).intersection(set(relation_pre)))\r\n","                    precision = correct / (len(relation_pre) + 1e-5)\r\n","                    recall = correct / (len(relation_true) + 1e-5)\r\n","                    f1 += (2 * precision * recall) / (precision + recall + 1e-5)\r\n","            f1_all += f1 / batch_size * 100.0  # f1 and acc of the current batch\r\n","            acc_all += f1_score(class_true, class_pre, average=\"macro\") * 100.0\r\n","            print(\"{} batch, f1-{:.4f}, acc-{:.4f}.\".format(i_batch, f1 / batch_size,\r\n","                                                            f1_score(class_true, class_pre, average=\"macro\")))\r\n","        # get all f1 and accuracy\r\n","        f1_all = f1_all / (i_batch + 1)\r\n","        acc_all = acc_all / (i_batch + 1)\r\n","        print(\"f1_all-{:.4f}, acc_all-{:.4f}\".format(f1_all, acc_all))\r\n","    return f1_all, acc_all\r\n","\r\n","\r\n","# Prediction (in test step)\r\n","##########################################################################################\r\n","def predict_ner(doc):\r\n","    headers = {\"Content-Type\": \"application/json\"}\r\n","    url = \"your-ner-model-url\"\r\n","    text = {\"text\": doc}\r\n","    result = requests.request(\"POST\", url, json=text, headers=headers)\r\n","    lst_doc = result.json()\r\n","    return lst_doc\r\n","\r\n","\r\n","def id2token(tokenizer, lst_ids):\r\n","    lst_text = tokenizer.convert_ids_to_tokens(lst_ids)\r\n","    lst_head = lst_text[lst_text.index(\"[unused42]\"):lst_text.index(\"[unused43]\")]\r\n","    lst_tail = lst_text[lst_text.index(\"[unused44]\"):lst_text.index(\"[unused45]\")]\r\n","    lst_head, lst_tail = lst_head[1:], lst_tail[1:]\r\n","    return lst_text, \"\".join(lst_head), \"\".join(lst_tail)\r\n","\r\n","\r\n","# predict each input sentence\r\n","def predict_span(tokenizer, model, input_data_gen):\r\n","    lst_pre = list()\r\n","    model.eval()\r\n","    with torch.no_grad():\r\n","        start_logits, end_logits, target_logits = model(input_ids=input_data_gen['input_ids'],\r\n","                                                        token_type_ids=input_data_gen['input_segments'],\r\n","                                                        attention_mask=input_data_gen['input_mask'])\r\n","        # post-processing\r\n","        start_logits = start_logits.detach().cpu().numpy()  # [bs, len]\r\n","        end_logits = end_logits.detach().cpu().numpy()  # [bs, len]\r\n","        start_pre = np.argmax(start_logits, axis=-1)  # [bs,]\r\n","        end_pre = np.argmax(end_logits, axis=-1)  # [bs,]\r\n","        class_pre = target_logits.detach().cpu().numpy()  # [bs,]\r\n","\r\n","        # calculate each test sample\r\n","        for i in range(target_logits.shape[0]):\r\n","            lst_ids = input_data_gen[\"input_ids\"][i].detach().cpu().tolist()\r\n","            lst_text, head_now, tail_now = id2token(tokenizer, lst_ids)\r\n","            class_pre_now = 1 if class_pre[i] > 0.5 else 0\r\n","            relation_now = None\r\n","            if class_pre_now == 1:  # 两个候选实体有关系，lst_pre只存有关系的三元组/二元组\r\n","                if start_pre[i] != end_pre[i]:\r\n","                    relation_now = \"\".join(lst_text[start_pre[i]:end_pre[i]])  # 两个候选实体有关系，且关系有span\r\n","                lst_pre.append((head_now, tail_now, relation_now))\r\n","        return lst_pre\r\n","\r\n","\r\n","def predict_now(doc, args=None, tokenizer=None, model=None, is_cuda=True, is_print=False):\r\n","    assert args and tokenizer and model\r\n","    print(\"***** Predict *****\")\r\n","    t_predict = time()\r\n","    pre_all = list()\r\n","    print(\"NER model ...\")\r\n","    lst_doc = predict_ner(doc)\r\n","    print(\"Classify model ...\")\r\n","    for piece in lst_doc:\r\n","        input_data, max_seq_len = string2token(tokenizer, text=piece[\"text\"], lst_entities=piece[\"entity\"],\r\n","                                               max_seq_length=args.max_seq_length)\r\n","        \r\n","        if input_data:\r\n","            input_data_gen = gen_batch_test(input_data, max_seq_len, is_cuda)\r\n","            pre_piece = predict_span(tokenizer, model, input_data_gen)\r\n","            if len(pre_piece) > 0:\r\n","                pre_all.extend(pre_piece)\r\n","    print(\"predicting time: {}\".format(time() - t_predict))\r\n","    if is_print:\r\n","        print(doc)\r\n","        for fact in pre_all:\r\n","            if fact[2]:\r\n","                print(fact[0], \" \", fact[2], \" \", fact[1])\r\n","            else:\r\n","                print(fact[0], \" 和 \", fact[1], \" 有关系\")\r\n","    return pre_all\r\n","\r\n","\r\n","def predict_one(sentence, args=None, tokenizer=None, model=None, is_cuda=True, is_print=True):\r\n","    assert args and tokenizer and model\r\n","    assert isinstance(sentence, str)\r\n","    label_pre = predict_now(sentence, args, tokenizer, model, is_cuda, is_print=is_print)\r\n","    return label_pre\r\n","\r\n","\r\n","# Measurement (in test step)\r\n","##########################################################################################\r\n","def get_label(lst):\r\n","    lst_span = list()\r\n","    lst_classify = list()\r\n","    for i in lst:\r\n","        lst_classify.append(str(i[0]) + \"_\" + str(i[1]))\r\n","        if len(i) == 3 and i[2]:\r\n","            lst_span.append(str(i[0]) + \"_\" + str(i[1]) + \"_\" + str(i[2]))\r\n","        else:\r\n","            lst_span.append(str(i[0]) + \"_\" + str(i[1]) + \"_NONE\")\r\n","    lst_span = set(lst_span)\r\n","    lst_classify = set(lst_classify)\r\n","    return lst_span, lst_classify\r\n","\r\n","\r\n","def get_metric(label_pre, label_true, sample, args):\r\n","    span_pre, classify_pre = get_label(label_pre)\r\n","    span_true, classify_true = get_label(label_true)\r\n","    span_inter = set.intersection(span_pre, span_true)\r\n","    classify_inter = set.intersection(classify_pre, classify_true)\r\n","\r\n","    # record\r\n","    with open(args.test_log, \"a\") as f_log:\r\n","        if len(span_inter) == 0 or len(classify_inter) == 0:\r\n","            f_log.write(\"【EXTRACTION FAILED】\\ns\")\r\n","        f_log.write(\"Text: \" + sample.get(\"text\", \"\") + \"\\n\")\r\n","        f_log.write(\"Pre: \" + \"，\".join(list(span_pre)) + \"\\n\")\r\n","        f_log.write(\"True: \" + \"，\".join(list(span_true)) + \"\\n\")\r\n","        f_log.write(\"Intersection Span: \" + \"，\".join(list(span_inter)) + \"\\n\")\r\n","        f_log.write(\"Intersection Classify: \" + \"，\".join(list(classify_inter)) + \"\\n\")\r\n","        f_log.write(\"\\n\")\r\n","\r\n","    span_recall = len(span_inter) / (len(span_true) + 1e-5)\r\n","    span_precision = len(span_inter) / (len(span_pre) + 1e-5)\r\n","    span_f1 = (2 * span_precision * span_recall) / (span_precision + span_recall + 1e-5)\r\n","\r\n","    classify_recall = len(classify_inter) / (len(classify_true) + 1e-5)\r\n","    classify_precision = len(classify_inter) / (len(classify_pre) + 1e-5)\r\n","    classify_f1 = (2 * classify_precision * classify_recall) / (classify_precision + classify_recall + 1e-5)\r\n","\r\n","    res = {\"span_precision\": span_precision,\r\n","           \"span_recall\": span_recall,\r\n","           \"span_f1\": span_f1,\r\n","           \"classify_recall\": classify_recall,\r\n","           \"classify_precision\": classify_precision,\r\n","           \"classify_f1\": classify_f1}\r\n","\r\n","    return res\r\n","\r\n","\r\n","def predict_all(load_path, args=None, tokenizer=None, model=None, is_cuda=True):\r\n","    assert args and tokenizer and model\r\n","    assert load_path.endswith(\".json\")\r\n","    with open(load_path, \"rb\") as f:\r\n","        d = json.load(f)\r\n","        span_f1, span_recall, span_precision = 0.0, 0.0, 0.0\r\n","        classify_f1, classify_recall, classify_precision = 0.0, 0.0, 0.0\r\n","        count_sample = 0\r\n","        for sample in d:\r\n","            if sample.get(\"text\", None):\r\n","                print(\"The {}th sample ...\".format(count_sample))\r\n","                label_pre = predict_now(sample.get(\"text\", None), args, tokenizer, model, is_cuda)\r\n","                label_true = sample.get(\"triples\", None)\r\n","                res_now = get_metric(label_pre, label_true, sample, args)\r\n","                span_f1 += res_now[\"span_f1\"]\r\n","                span_recall += res_now[\"span_recall\"]\r\n","                span_precision += res_now[\"span_precision\"]\r\n","                classify_f1 += res_now[\"classify_f1\"]\r\n","                classify_recall += res_now[\"classify_recall\"]\r\n","                classify_precision += res_now[\"classify_precision\"]\r\n","                count_sample += 1\r\n","    span_f1 /= float(count_sample)\r\n","    span_recall /= float(count_sample)\r\n","    span_precision /= float(count_sample)\r\n","    classify_f1 /= float(count_sample)\r\n","    classify_recall /= float(count_sample)\r\n","    classify_precision /= float(count_sample)\r\n","\r\n","    # record\r\n","    with open(args.test_log, \"a\") as f_log:\r\n","        f_log.write(\"Span-F1: %s \\n\" % str(span_f1))\r\n","        f_log.write(\"Span-Recall:  %s \\n\" % str(span_recall))\r\n","        f_log.write(\"Span-Precision:  %s \\n\" % str(span_precision))\r\n","        f_log.write(\"Classify-F1:  %s \\n\" % str(classify_f1))\r\n","        f_log.write(\"Classify-Recall:  %s \\n\" % str(classify_recall))\r\n","        f_log.write(\"Classify-Precision:  %s \\n\" % str(classify_precision))\r\n","        f_log.write(\"\\n\")\r\n","\r\n","    return span_f1, span_recall, span_precision, classify_f1, classify_recall, classify_precision, count_sample - 1\r\n","\r\n","\r\n","# Main\r\n","##########################################################################################\r\n","if __name__ == \"__main__\":\r\n","    s = \"\"\"赛尔提是本作主角，来自爱尔兰的无头骑士，性别常被认错，但确实为女性。赛尔提本来是抱着头、驾着无头马的妖精。\r\n","               赛尔提乘坐的黑摩托车，是一匹马变形而成的。二十多年前，岸谷森严使用妖刀罪歌得到她的头。\r\n","               陷于迷茫的她为了找回头于是离开爱尔兰追到了日本池袋。\r\n","               赛尔提来到池袋后平时是在作运输、保镖之类的工作，并成为当地有名的都市传说。\r\n","               赛尔提在渡船上遇上新罗父子，结识后住进了他们家中，就这样与新罗同居至今。\r\n","               赛尔提喜欢新罗，是DOLLARS的一员，少数知道首领身份的人。赛尔提是羽岛幽平和圣边琉璃的粉丝。\"\"\"\r\n","\r\n","    # create train & dev samples from raw data\r\n","    raw2json(tokenizer,\r\n","             load_path=args.load_train_path,\r\n","             save_path=args.suffix_name,\r\n","             max_lines=args.max_lines,\r\n","             max_seq_length=args.max_seq_length,\r\n","             train_split=args.train_split,\r\n","             blank_ratio=args.blank_ratio,\r\n","             num_relation=args.num_relation,\r\n","             repeat_time=args.repeat_time)\r\n","\r\n","    # train & evaluate model\r\n","    train(args=args, tokenizer=tokenizer, model=model, is_cuda=is_cuda, n_gpu=n_gpu)\r\n","\r\n","    # predict only one sample:\r\n","    result = predict_one(s, args=args, tokenizer=tokenizer, model=model, is_cuda=is_cuda)\r\n","    for i in result:\r\n","        print(i)\r\n","\r\n","    # predict samples:\r\n","    results = predict_all(load_path=args.load_test_path,\r\n","                          args=args,\r\n","                          tokenizer=tokenizer,\r\n","                          model=model,\r\n","                          is_cuda=is_cuda)\r\n","    print(\"Result of Model {} on {} test samples: Span-F1-{},R-{},P-{} | Classify-F1-{},R-{},P-{}\"\r\n","          .format(args.model_name + \"_\" + args.suffix_name,\r\n","                  results[6], results[0], results[1], results[2], results[3], results[4], results[5]))\r\n"],"execution_count":null,"outputs":[]}]}